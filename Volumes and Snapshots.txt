Volumes and Snapshots


- Volumes exist in EBS
  - Virtual Hard Disk
- Snapshots exist in S3
- Snapshots are point in time copies of Volumes
- Snapshots are incremental,this means that only the blocks that have changed
  since your last snapshot are moved to S3
- if this is your snapshot it may take some time to create 
- Volumes and Snapshots can be taken within the Availability ZOne
- Across Availability ZOne its not permitted
- Where the Root device is there,we can take there only the Snapshots

Create an AMI linux Instance with normal setup steps
EC2 Dashboard -> Volumes -> You can see the Volume of the Instance there 
                            (and that has been attached to the AMI Linux Instance
							OS installed on this)
							
		Lets Create a New Virtual Disk 
		-> Create Volume -> Different types Available, Select General Purpose GP2 (SSD) 100GB
		            -> Keep it under same Availability ZOne as ur AMI linux Volume Exists
					-> Encryption Leave it
					-> So Two Volumes you can see in the list
					-> Actions -> Attach a Volume-> Choose the Instance(AMI Linux)
SSH into the AMI Linux Instance


[ec2-user@ip-172-31-9-219 ~]$ lsblk
NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
xvda    202:0    0   8G  0 disk
└─xvda1 202:1    0   8G  0 part /
[ec2-user@ip-172-31-9-219 ~]$
After attaching the new Volume, you can see that in tke List 'xvdf'
[ec2-user@ip-172-31-9-219 ~]$ lsblk
NAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
xvda    202:0    0    8G  0 disk
└─xvda1 202:1    0    8G  0 part /
xvdf    202:80   0  100G  0 disk
[ec2-user@ip-172-31-9-219 ~]$
[ec2-user@ip-172-31-9-219 ~]$

[ec2-user@ip-172-31-9-219 ~]$ sudo su
[root@ip-172-31-9-219 ec2-user]# cd /
[root@ip-172-31-9-219 /]# mkdir extravolume
[root@ip-172-31-9-219 /]# file -s /dev/xvdf
/dev/xvdf: data
[root@ip-172-31-9-219 /]#
[root@ip-172-31-9-219 /]# mkfs -t ext4 /dev/xvdf
mke2fs 1.42.12 (29-Aug-2014)
Creating filesystem with 26214400 4k blocks and 6553600 inodes
Filesystem UUID: 154581d4-6677-40fb-b820-e35f1020c74a
Superblock backups stored on blocks:
        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208,
        4096000, 7962624, 11239424, 20480000, 23887872

Allocating group tables: done
Writing inode tables: done
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done

[root@ip-172-31-9-219 /]#
[root@ip-172-31-9-219 /]# mount /dev/xvdf /extravolume/
[root@ip-172-31-9-219 /]# cd extravolume/
[root@ip-172-31-9-219 extravolume]# ls -l
total 16
drwx------ 2 root root 16384 Aug 10 09:14 lost+found
[root@ip-172-31-9-219 extravolume]#
[root@ip-172-31-9-219 extravolume]# cat > test.txt
hi this is the extravolume
and we are gonna create a snapshot from this
and will get that mounted in new HD
[root@ip-172-31-9-219 extravolume]#
[root@ip-172-31-9-219 extravolume]# cat test.txt
hi this is the extravolume
and we are gonna create a snapshot from this
and will get that mounted in new HD
[root@ip-172-31-9-219 extravolume]#
[root@ip-172-31-9-219 extravolume]# lsblk
NAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
xvda    202:0    0    8G  0 disk
└─xvda1 202:1    0    8G  0 part /
xvdf    202:80   0  100G  0 disk /extravolume
[root@ip-172-31-9-219 extravolume]#
[root@ip-172-31-9-219 extravolume]#

UnMount it since we are going to take snapshot and destry this.
[root@ip-172-31-9-219 /]#
[root@ip-172-31-9-219 /]# umount /dev/xvdf
[root@ip-172-31-9-219 /]#

now just detach the Volume Newly Created. from the Actions' Menu
Now Create a Snapshot of this unattached Volume
From Actions' Menu Select 'Create Snapshot'
Once created,just chk it here 'EC2 Dashboard->EBS->Snapshots
(it will take sometime to get created)
You should see that 'extravolume' status has 'completed'

Now Just Go back to the EC2 Dashboard->EBS->Volumes and Delete the newly created Volume.
Refresh the screen to verify,its been removed completely
Chk the 'EC2 Dashboard->EBS->Snapshots' you wil still see 'extravolumesnapshot'

Now Click from the Snapshots Screen -> Actions -> Create New Volume
(make sure you have same Availability Zone Selected)
Come back to 'EC2 Dashboard->EBS->Volumes',you can see the new volume created.

chk it in the SSH session that,no new volumes as such 
[ec2-user@ip-172-31-9-219 ~]$ lsblk
NAME    MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
xvda    202:0    0   8G  0 disk
└─xvda1 202:1    0   8G  0 part /
[ec2-user@ip-172-31-9-219 ~]$

Now Attach the Newly Volume to the EC2 Instance from the GUI
then you can see the new volume is listed

[ec2-user@ip-172-31-9-219 ~]$ lsblk
NAME    MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
xvda    202:0    0    8G  0 disk
└─xvda1 202:1    0    8G  0 part /
xvdf    202:80   0  100G  0 disk
[ec2-user@ip-172-31-9-219 ~]$
[ec2-user@ip-172-31-9-219 ~]$
[ec2-user@ip-172-31-9-219 ~]$ cd /
[ec2-user@ip-172-31-9-219 /]$ cd extravolume/
[ec2-user@ip-172-31-9-219 extravolume]$
[ec2-user@ip-172-31-9-219 extravolume]$ sudo su
[root@ip-172-31-9-219 extravolume]# file -s /dev/xvdf
/dev/xvdf: Linux rev 1.0 ext4 filesystem data, UUID=154581d4-6677-40fb-b820-e35f1020c74a (extents) (large files) (huge files)
[root@ip-172-31-9-219 extravolume]#

Once you mount it to the same dir' you can see the contents which are earlier made
on the previous Volume

[root@ip-172-31-9-219 /]# mount /dev/xvdf /extravolume/
[root@ip-172-31-9-219 /]# cd extravolume/
[root@ip-172-31-9-219 extravolume]# ls -l
total 20
drwx------ 2 root root 16384 Aug 10 09:14 lost+found
-rw-r--r-- 1 root root   110 Aug 10 09:16 test.txt
[root@ip-172-31-9-219 extravolume]#
[root@ip-172-31-9-219 extravolume]#
[root@ip-172-31-9-219 extravolume]# cat test.txt
hi this is the extravolume
and we are gonna create a snapshot from this
and will get that mounted in new HD
[root@ip-172-31-9-219 extravolume]#
______________________________________________________________________________________

Removal Procedure
 - Delete the Snapshot,then detach the newly created Volume
 - then delete the newly created Volume
 - then Terminate the AMI Linux Instance
 - verify all are cleaned
______________________________________________________________________________________


